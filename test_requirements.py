"""æµ‹è¯•ä¸‰ä¸ªæ ¸å¿ƒè¦æ±‚"""
import requests
import json

print("="*60)
print("æµ‹è¯•æ ¸å¿ƒè¦æ±‚")
print("="*60)

# è¦æ±‚1: èŠ‚ç‚¹å®šä¹‰ä½¿ç”¨Wikipedia API
print("\nã€è¦æ±‚1ã€‘éªŒè¯èŠ‚ç‚¹å®šä¹‰æ¥è‡ªWikipedia API")
print("-"*60)
r = requests.post('http://localhost:8888/api/v1/discover', 
    json={'concept': 'æœºå™¨å­¦ä¹ ', 'depth': 1, 'max_concepts': 10}, 
    timeout=120)
data = r.json()
nodes = data.get('data', {}).get('nodes', [])

total_nodes = len(nodes)
wiki_nodes = sum(1 for n in nodes if n.get('source') == 'Wikipedia')
llm_nodes = sum(1 for n in nodes if n.get('source') == 'LLM')

print(f"æ€»èŠ‚ç‚¹æ•°: {total_nodes}")
print(f"Wikipediaæ¥æº: {wiki_nodes} ({wiki_nodes/total_nodes*100:.1f}%)")
print(f"LLMæ¥æº: {llm_nodes} ({llm_nodes/total_nodes*100:.1f}%)")

if nodes:
    print(f"\nç¤ºä¾‹èŠ‚ç‚¹:")
    for i, node in enumerate(nodes[:3], 1):
        print(f"  {i}. {node.get('label')}")
        print(f"     æ¥æº: {node.get('source')}")
        print(f"     å®šä¹‰: {node.get('definition', '')[:100]}...")
        if node.get('wiki_url'):
            print(f"     é“¾æ¥: {node.get('wiki_url')}")

result1 = "âœ… é€šè¿‡" if wiki_nodes > llm_nodes else "âŒ å¤±è´¥"
print(f"\nç»“æœ: {result1} - Wikipediaå®šä¹‰å æ¯” {wiki_nodes/total_nodes*100:.1f}%")

# è¦æ±‚2: ç›¸å…³æ¦‚å¿µå±•å¼€æ˜¾ç¤ºå¤§æ¨¡å‹ç”Ÿæˆçš„è¯¦ç»†ä»‹ç»
print("\nã€è¦æ±‚2ã€‘éªŒè¯ç›¸å…³æ¦‚å¿µå±•å¼€åŠŸèƒ½")
print("-"*60)
r = requests.get('http://localhost:8888/api/v1/concept/ç¥ç»ç½‘ç»œ/detail', timeout=60)
detail_data = r.json().get('data', {})

has_wiki_def = bool(detail_data.get('wiki_definition'))
has_detailed_intro = bool(detail_data.get('detailed_introduction'))
intro_length = len(detail_data.get('detailed_introduction', ''))

print(f"æ¦‚å¿µåç§°: {detail_data.get('concept')}")
print(f"Wikipediaå®šä¹‰: {'æœ‰' if has_wiki_def else 'æ— '} ({len(detail_data.get('wiki_definition', ''))} å­—ç¬¦)")
print(f"è¯¦ç»†ä»‹ç»: {'æœ‰' if has_detailed_intro else 'æ— '} ({intro_length} å­—ç¬¦)")
print(f"ç›¸å…³è®ºæ–‡æ•°: {detail_data.get('papers_count', 0)}")

if has_detailed_intro:
    print(f"\nè¯¦ç»†ä»‹ç»å†…å®¹é¢„è§ˆ (å‰300å­—):")
    print(detail_data.get('detailed_introduction', '')[:300])
    print("...")

result2 = "âœ… é€šè¿‡" if has_detailed_intro and intro_length > 200 else "âŒ å¤±è´¥"
print(f"\nç»“æœ: {result2} - è¯¦ç»†ä»‹ç»é•¿åº¦ {intro_length} å­—ç¬¦")

# è¦æ±‚3: éªŒè¯Arxiv APIæ£€ç´¢åŠŸèƒ½
print("\nã€è¦æ±‚3ã€‘éªŒè¯Arxiv APIæ£€ç´¢åŠŸèƒ½")
print("-"*60)

test_queries = ["deep learning", "neural networks", "quantum computing"]
all_success = True

for query in test_queries:
    r = requests.get('http://localhost:8888/api/v1/arxiv/search', 
        params={'query': query, 'max_results': 3})
    arxiv_data = r.json()
    
    if arxiv_data.get('status') == 'success':
        papers = arxiv_data.get('data', {}).get('papers', [])
        print(f"âœ“ '{query}': æ£€ç´¢åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        if papers:
            print(f"  ç¤ºä¾‹: {papers[0].get('title')[:60]}...")
    else:
        print(f"âœ— '{query}': æ£€ç´¢å¤±è´¥")
        all_success = False

result3 = "âœ… é€šè¿‡" if all_success else "âŒ å¤±è´¥"
print(f"\nç»“æœ: {result3} - Arxiv APIæ­£å¸¸å·¥ä½œ")

# æ€»ç»“
print("\n" + "="*60)
print("æµ‹è¯•æ€»ç»“")
print("="*60)
print(f"è¦æ±‚1 - Wikipediaå®šä¹‰: {result1}")
print(f"è¦æ±‚2 - è¯¦ç»†æ¦‚å¿µå±•å¼€: {result2}")
print(f"è¦æ±‚3 - Arxiv API: {result3}")

all_passed = all(r == "âœ… é€šè¿‡" for r in [result1, result2, result3])
print(f"\næ€»ä½“ç»“æœ: {'ğŸ‰ å…¨éƒ¨é€šè¿‡' if all_passed else 'âš ï¸ éƒ¨åˆ†å¤±è´¥'}")
print("="*60)
